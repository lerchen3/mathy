TODO: finetune QwQ-32b on obscure math because it's pretty shit at that (and I know it's 
not in QwQ's training data). o1-api calls with some fsp on the language of QwQ would suffice
for data collection. (We'd have to scrape the pdfs all over again for the solutions. Maybe
I should have scraped those the first time around. Whoops. Whatever, only took 15 hours of
compute.)

Holding off because I don't want to spend a lot of money on this just for a better
model than QwQ to to come out.