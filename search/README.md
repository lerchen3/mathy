two big ideas:
1) early pruning gives you more tries. Testing Qwen-32b-Instruct, MathBERT-encoder + NN, and if I feel like it, a fine-tuned version of Qwen-32-Instruct (the task is simply, given a prefix of a solution, predict if it will go on to find a final answer within the token limit of 16384.) (basically a discrete MCTS, I guess. idk I obviously don't trust an LLM to give me a continuous value function.)
2) There's a billion different prompt heads you can try. I used Claude to generate 50 prompt heads & am trying to use a MathBERT-encoder + NN to predict which prompt heads will work best. Obviously could use a lot lot more prompt heads, containing techniques or whatever, but looks like this works fine given this won the early sharing prize.

These are independent of each other; "early pruning.py" incorporates early pruning with prompt heads randomly chosen, while "nn prompt selection.py" incorporates prompt heads chosen by a MathBERT-encoder + NN.